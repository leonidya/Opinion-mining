{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb4f336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T08:14:50.007184Z",
     "start_time": "2023-01-12T08:14:46.934210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   polarity          id                          date     query  \\\n",
      "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "\n",
      "              user                                               text  \n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
      "\n",
      " The shape data is:(1600000, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the Sentiment140 dataset as a data frame\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding = 'latin1', names=['polarity', 'id', 'date', 'query', 'user', 'text'])\n",
    "# Print the first 5 rows of the data frame\n",
    "print(df.head())\n",
    "print(f\"\\n The shape data is:{df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eeb7d2e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:06:32.956223Z",
     "start_time": "2023-01-12T15:06:32.394673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count    Dtype \n",
      "---  ------    --------------    ----- \n",
      " 0   polarity  1600000 non-null  int64 \n",
      " 1   id        1600000 non-null  int64 \n",
      " 2   date      1600000 non-null  object\n",
      " 3   query     1600000 non-null  object\n",
      " 4   user      1600000 non-null  object\n",
      " 5   text      1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aec0d530",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:08:11.547110Z",
     "start_time": "2023-01-12T15:08:11.514189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    800000\n",
       "4    800000\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4171057b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T14:43:53.128160Z",
     "start_time": "2023-01-12T14:43:52.827621Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799995</th>\n",
       "      <td>0</td>\n",
       "      <td>2329205009</td>\n",
       "      <td>Thu Jun 25 10:28:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>dandykim</td>\n",
       "      <td>Sick  Spending my day laying in bed listening ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799996</th>\n",
       "      <td>0</td>\n",
       "      <td>2329205038</td>\n",
       "      <td>Thu Jun 25 10:28:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bigenya</td>\n",
       "      <td>Gmail is down?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799997</th>\n",
       "      <td>0</td>\n",
       "      <td>2329205473</td>\n",
       "      <td>Thu Jun 25 10:28:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LeeLHoke</td>\n",
       "      <td>rest in peace Farrah! So sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799998</th>\n",
       "      <td>0</td>\n",
       "      <td>2329205574</td>\n",
       "      <td>Thu Jun 25 10:28:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>davidlmulder</td>\n",
       "      <td>@Eric_Urbane Sounds like a rival is flagging y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799999</th>\n",
       "      <td>0</td>\n",
       "      <td>2329205794</td>\n",
       "      <td>Thu Jun 25 10:28:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tpchandler</td>\n",
       "      <td>has to resit exams over summer...  wishes he w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        polarity          id                          date     query  \\\n",
       "0              0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1              0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2              0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3              0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4              0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...          ...         ...                           ...       ...   \n",
       "799995         0  2329205009  Thu Jun 25 10:28:28 PDT 2009  NO_QUERY   \n",
       "799996         0  2329205038  Thu Jun 25 10:28:28 PDT 2009  NO_QUERY   \n",
       "799997         0  2329205473  Thu Jun 25 10:28:30 PDT 2009  NO_QUERY   \n",
       "799998         0  2329205574  Thu Jun 25 10:28:30 PDT 2009  NO_QUERY   \n",
       "799999         0  2329205794  Thu Jun 25 10:28:31 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \n",
       "0       _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1         scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2              mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3               ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                 ...                                                ...  \n",
       "799995         dandykim  Sick  Spending my day laying in bed listening ...  \n",
       "799996          bigenya                                    Gmail is down?   \n",
       "799997         LeeLHoke                      rest in peace Farrah! So sad   \n",
       "799998     davidlmulder  @Eric_Urbane Sounds like a rival is flagging y...  \n",
       "799999       tpchandler  has to resit exams over summer...  wishes he w...  \n",
       "\n",
       "[800000 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['polarity']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09353bc0",
   "metadata": {},
   "source": [
    "### Preprocess the data: lowercasing, tokenization, removing stop words and punctuation. It's important to note, that I test two preprocessing options, with and without removing stop words and punctuation, stemming, in order to understant the impact on sentiment analysis. This is part one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae99a44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T08:14:51.969161Z",
     "start_time": "2023-01-12T08:14:51.549699Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd, os, pickle\n",
    "import sklearn.feature_extraction.text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17cf288a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T08:21:41.822525Z",
     "start_time": "2023-01-12T08:21:41.812520Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name_opnion_encoded  = 'opnion_encoded_text_ready.pickle'\n",
    "file_name_vectorizer_pickle = 'vectorizer.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "170ec567",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T08:35:59.667921Z",
     "start_time": "2023-01-12T08:22:33.116239Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(file_name_opnion_encoded):\n",
    "    # Get the 'text' column from the data frame\n",
    "    texts = df['text']\n",
    "\n",
    "    # Preprocess the texts\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        # Lowercase the text\n",
    "        text = text.lower()\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # Remove stop words\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "        # Remove punctuation\n",
    "        punctuation = string.punctuation\n",
    "        tokens = [token for token in tokens if token not in punctuation]\n",
    "\n",
    "        # Stem or lemmatize the tokens\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        # Join the tokens back into a single string\n",
    "        processed_text = ' '.join(tokens)\n",
    "\n",
    "        # Append the processed text to the list\n",
    "        processed_texts.append(processed_text)\n",
    "\n",
    "    # Use CountVectorizer to encode the texts\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    encoded_texts = vectorizer.fit_transform(processed_texts)\n",
    "    with open(file_name_vectorizer_pickle, 'w+b') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    with open(file_name_opnion_encoded, 'w+b') as f: \n",
    "        pickle.dump(encoded_texts,f, -1)  \n",
    "else:\n",
    "    with open(file_name_opnion_encoded, 'rb') as f:\n",
    "        encoded_texts = pickle.load(f)\n",
    "    with open(file_name_vectorizer_pickle, 'rb') as f:\n",
    "        vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27cd0290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T09:40:42.911952Z",
     "start_time": "2023-01-12T09:39:29.086686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78943125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leon\\anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model as lm\n",
    "from sklearn.model_selection import train_test_split\n",
    "file_name_model  = 'model_logistic_regression.pickle'\n",
    "# Get the labels for the text\n",
    "if not os.path.isfile(file_name_model):\n",
    "    labels = df['polarity']\n",
    "    # Split the data into a training set and a test set\n",
    "    features_train, features_test, labels_train, labels_test = train_test_split(encoded_texts, labels, test_size=0.2, random_state=42)\n",
    "    # Train the logistic regression model\n",
    "    model_logistic = lm.LogisticRegression()\n",
    "    model_logistic.fit(features_train, labels_train)\n",
    "    # Evaluate the model on the test set\n",
    "    accuracy = model_logistic.score(features_test, labels_test)\n",
    "    print('Accuracy:', accuracy)\n",
    "else:\n",
    "    with open(file_name_model, 'rb') as f:\n",
    "        model_logistic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c295f8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T09:40:43.214020Z",
     "start_time": "2023-01-12T09:40:42.913954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7799105478941484\n",
      "Recall: 0.8082875406526859\n",
      "F1 score: 0.7938455325346029\n"
     ]
    }
   ],
   "source": [
    "import nbformat.v4 as nbformat\n",
    "from sklearn import metrics\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model_logistic.predict(features_test)\n",
    "\n",
    "# Compute the precision, recall, and F1 score\n",
    "precision = metrics.precision_score(labels_test, predictions, pos_label=4)\n",
    "recall = metrics.recall_score(labels_test, predictions, pos_label=4)\n",
    "f1 = metrics.f1_score(labels_test, predictions, pos_label=4)\n",
    "\n",
    "# Print the results\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e50efce1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T09:40:43.230043Z",
     "start_time": "2023-01-12T09:40:43.216020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0]\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "text = [\"fraud\"]\n",
    "# Transform the text using the same vectorizer\n",
    "predict_text_train = vectorizer.transform(text)\n",
    "prediction = model_logistic.predict(predict_text_train)\n",
    "print('Prediction:', prediction)\n",
    "if prediction == 4:\n",
    "    print('positive')\n",
    "# elif prediction == 2:\n",
    "#     print('neutral')\n",
    "# becouse it's logistic regresion it will be 1 or 0 (in this case it's 4 - positive or 0 negative)\n",
    "elif prediction == 0:\n",
    "    print('negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79647f94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T14:10:46.164485Z",
     "start_time": "2023-01-12T14:10:46.131238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [4]\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "text = [\"It's too good to be true\"]\n",
    "# Transform the text using the same vectorizer\n",
    "predict_text_train = vectorizer.transform(text)\n",
    "prediction = model_logistic.predict(predict_text_train)\n",
    "print('Prediction:', prediction)\n",
    "if prediction == 4:\n",
    "    print('positive')\n",
    "# elif prediction == 2:\n",
    "#     print('neutral')\n",
    "# no 2 in the data - and if there was 2, you should use model_logistic_regression_multi_class\n",
    "elif prediction == 0:\n",
    "    print('negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca042da",
   "metadata": {},
   "source": [
    "# ----------------------------------PART 2 - flair ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a86b096e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-10T15:21:53.213371Z",
     "start_time": "2023-01-10T15:19:58.108957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (4.21.3)\n",
      "Collecting flair\n",
      "  Downloading flair-0.11.3-py3-none-any.whl (401 kB)\n",
      "     ------------------------------------ 401.9/401.9 kB 309.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\leon\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: requests in c:\\users\\leon\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\leon\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\leon\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Collecting wikipedia-api\n",
      "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ------------------------------------ 981.5/981.5 kB 748.8 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting more-itertools\n",
      "  Downloading more_itertools-9.0.0-py3-none-any.whl (52 kB)\n",
      "     ---------------------------------------- 52.8/52.8 kB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from flair) (1.2.0)\n",
      "Collecting hyperopt>=0.2.7\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 751.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: lxml in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from flair) (4.9.1)\n",
      "Collecting mpld3==0.3\n",
      "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
      "     ------------------------------------ 788.5/788.5 kB 206.7 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting bpemb>=0.3.2\n",
      "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Collecting janome\n",
      "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "     ---------------------------------------- 19.7/19.7 MB 1.0 MB/s eta 0:00:00\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sentencepiece==0.1.95\n",
      "  Downloading sentencepiece-0.1.95-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 1.2 MB/s eta 0:00:00\n",
      "Collecting deprecated>=1.2.4\n",
      "  Using cached Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "     -------------------------------------- 53.1/53.1 kB 130.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from flair) (3.6.2)\n",
      "Collecting conllu>=4.0\n",
      "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting gensim>=3.4.0\n",
      "  Downloading gensim-4.3.0-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "     ---------------------------------------- 24.0/24.0 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting segtok>=1.5.7\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting gdown==4.4.0\n",
      "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from flair) (2.8.2)\n",
      "Collecting pptree\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from flair) (1.13.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from gdown==4.4.0->flair) (4.11.1)\n",
      "Requirement already satisfied: six in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from gdown==4.4.0->flair) (1.16.0)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\leon\\appdata\\roaming\\python\\python39\\site-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from gensim>=3.4.0->flair) (1.9.3)\n",
      "Collecting FuzzyTM>=0.4.0\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Collecting Cython==0.29.32\n",
      "  Downloading Cython-0.29.32-py2.py3-none-any.whl (986 kB)\n",
      "     ------------------------------------ 986.3/986.3 kB 753.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\leon\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "     -------------------------------------- 200.5/200.5 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting future\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting networkx>=2.2\n",
      "  Downloading networkx-3.0-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 985.8 kB/s eta 0:00:00\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
      "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\leon\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.0.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib>=2.2.3->flair) (9.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\leon\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leon\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\leon\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\leon\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\leon\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from ftfy->flair) (0.2.5)\n",
      "Collecting pyfume\n",
      "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "     -------------------------------------- 67.1/67.1 kB 903.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim>=3.4.0->flair) (1.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from beautifulsoup4->gdown==4.4.0->flair) (2.3.2.post1)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\leon\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim>=3.4.0->flair) (2022.6)\n",
      "Collecting fst-pso\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting simpful\n",
      "  Downloading simpful-2.9.0-py3-none-any.whl (30 kB)\n",
      "Collecting miniful\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: gdown, mpld3, sqlitedict, langdetect, pptree, overrides, future, fst-pso, miniful\n",
      "  Building wheel for gdown (pyproject.toml): started\n",
      "  Building wheel for gdown (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14785 sha256=20f776b7981b9d719d25dcf8ccd3946569df26122bad4a559698b3794dccc640\n",
      "  Stored in directory: c:\\users\\leon\\appdata\\local\\pip\\cache\\wheels\\91\\79\\92\\b7383aa7cb8c51320976217882671ae0ed8e5cddead7b90024\n",
      "  Building wheel for mpld3 (setup.py): started\n",
      "  Building wheel for mpld3 (setup.py): finished with status 'done'\n",
      "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116720 sha256=8857fa1ceb7fcbe337650a8a7ae87a4651a7f27ff1ee6a48efc71eea8db77b83\n",
      "  Stored in directory: c:\\users\\leon\\appdata\\local\\pip\\cache\\wheels\\2b\\29\\df\\6d418d98833fa10b93a4a62999fecc462e28e193a12d705460\n",
      "  Building wheel for sqlitedict (setup.py): started\n",
      "  Building wheel for sqlitedict (setup.py): finished with status 'done'\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16901 sha256=27bc77a264d58fcf94583728410b4fb73e0e13477e6a4253be5c47120cca9403\n",
      "  Stored in directory: c:\\users\\leon\\appdata\\local\\pip\\cache\\wheels\\23\\96\\9a\\a7c729ccc393f6ba66e52e60e8485fbfa44a0be1c399433ec0\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993254 sha256=015fca8ec4fc081b7ffd096280721ab056bf83899db3f8763c5da25fcbb7c989\n",
      "  Stored in directory: c:\\users\\leon\\appdata\\local\\pip\\cache\\wheels\\6a\\67\\f8\\9cf1a8ff87e0b37f738769df49cc142a655489a6d27b68089f\n",
      "  Building wheel for pptree (setup.py): started\n",
      "  Building wheel for pptree (setup.py): finished with status 'done'\n",
      "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4613 sha256=bf5da8870d6eb33cedb05c891778b7a2d903258deb8a70d8d98fed35a31e6add\n",
      "  Stored in directory: c:\\users\\leon\\appdata\\local\\pip\\cache\\wheels\\88\\68\\f2\\d4c9dcd99ee0e209aa1b1e2679f6fbff1100af289e665c3bf5\n",
      "  Building wheel for overrides (setup.py): started\n",
      "  Building wheel for overrides (setup.py): finished with status 'done'\n",
      "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10194 sha256=e9e4ad0be3bb1769781f4a079624df9e9309ba6674bb6924c477cb76def97629\n",
      "  Stored in directory: c:\\users\\leon\\appdata\\local\\pip\\cache\\wheels\\58\\03\\73\\9477948ce0cf1ef005cccc79e189655d1ca06d968ce9f6854f\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491086 sha256=22e6ec62aef0c37eccc544d75d65878e68018e048ebf3721ec13a545ddbec669\n",
      "  Stored in directory: c:\\users\\leon\\appdata\\local\\pip\\cache\\wheels\\96\\66\\19\\2de75120f5d0bc185e9d16cf0fd223d8471ed025de08e45867\n",
      "  Building wheel for fst-pso (setup.py): started\n",
      "  Building wheel for fst-pso (setup.py): finished with status 'done'\n",
      "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20449 sha256=4bdb5904ad4c9ed3b89fcf3629336c639219d7d43698f0718964e0a56142b2ea\n",
      "  Stored in directory: c:\\users\\leon\\appdata\\local\\pip\\cache\\wheels\\d7\\1e\\9c\\3c352867b1bb1ddba0e51149a0d06c1e50c1c9b1c5ebcac787\n",
      "  Building wheel for miniful (setup.py): started\n",
      "  Building wheel for miniful (setup.py): finished with status 'done'\n",
      "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3523 sha256=b0bdeb8c4a4f902d3cf6d3e88812b9a8f2d5cc8187b16a725cb49dcb93328fc0\n",
      "  Stored in directory: c:\\users\\leon\\appdata\\local\\pip\\cache\\wheels\\37\\45\\a2\\db878712db97aaab6f574d750789bc6c99e3b706af8af6b936\n",
      "Successfully built gdown mpld3 sqlitedict langdetect pptree overrides future fst-pso miniful\n",
      "Installing collected packages: sqlitedict, sentencepiece, py4j, pptree, overrides, mpld3, janome, wrapt, tabulate, segtok, PySocks, networkx, more-itertools, langdetect, importlib-metadata, future, ftfy, Cython, conllu, cloudpickle, wikipedia-api, simpful, miniful, konoha, hyperopt, deprecated, gdown, fst-pso, pyfume, FuzzyTM, gensim, bpemb, flair\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 5.0.0\n",
      "    Uninstalling importlib-metadata-5.0.0:\n",
      "      Successfully uninstalled importlib-metadata-5.0.0\n",
      "Successfully installed Cython-0.29.32 FuzzyTM-2.0.5 PySocks-1.7.1 bpemb-0.3.4 cloudpickle-2.2.0 conllu-4.5.2 deprecated-1.2.13 flair-0.11.3 fst-pso-1.8.1 ftfy-6.1.1 future-0.18.2 gdown-4.4.0 gensim-4.3.0 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 miniful-0.0.6 more-itertools-9.0.0 mpld3-0.3 networkx-3.0 overrides-3.1.0 pptree-3.1 py4j-0.10.9.7 pyfume-0.2.25 segtok-1.5.11 sentencepiece-0.1.95 simpful-2.9.0 sqlitedict-2.1.0 tabulate-0.9.0 wikipedia-api-0.5.8 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df671fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:05:52.052244Z",
     "start_time": "2023-01-12T15:05:36.950968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-12 17:05:42,922 loading file C:\\Users\\Leon\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "\n",
    "# Hugging Face model\n",
    "from transformers import pipeline\n",
    "\n",
    "# Import flair pre-trained sentiment model\n",
    "from flair.models import TextClassifier\n",
    "fl_classifier = TextClassifier.load('en-sentiment')\n",
    "\n",
    "# Import flair Sentence to process input text\n",
    "from flair.data import Sentence\n",
    "\n",
    "# Import accuracy_score to check performance\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6223772",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:08:56.887115Z",
     "start_time": "2023-01-12T15:08:56.874113Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to get Flair sentiment prediction score\n",
    "def score_flair(text):\n",
    "  # Flair tokenization\n",
    "  sentence = Sentence(text)\n",
    "  # Predict sentiment\n",
    "  fl_classifier.predict(sentence)\n",
    "  # Extract the score\n",
    "  score = sentence.labels[0].score\n",
    "  # Extract the predicted label\n",
    "  value = sentence.labels[0].value\n",
    "  # Return the score and the predicted label\n",
    "  return score, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cadde2f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:09:00.648827Z",
     "start_time": "2023-01-12T15:08:57.657835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9995369911193848, 'NEGATIVE')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_flair(\"scam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cca732d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:09:20.904704Z",
     "start_time": "2023-01-12T15:09:20.857774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9945777654647827, 'POSITIVE')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_flair(\"It's too good to be true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b875ab5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:12:29.841775Z",
     "start_time": "2023-01-12T15:12:29.833773Z"
    }
   },
   "source": [
    "After the function is defined, we can apply the function to each review in the dataset and create the predicted sentiments.\n",
    "\n",
    "From the score distribution, we can see that the minimum score is 0.53 and the average score is 0.99, indicating that the model is very confident about the sentiment predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4cb576c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:52:31.094400Z",
     "start_time": "2023-01-12T15:52:31.089890Z"
    }
   },
   "outputs": [],
   "source": [
    "data = df\n",
    "part_of_data = data[1:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d9fb3f53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:54:38.644360Z",
     "start_time": "2023-01-12T15:52:35.018554Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_21204\\4207717770.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  part_of_data['scores_flair'] = part_of_data['text'].apply(lambda s: score_flair(s)[0])\n",
      "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_21204\\4207717770.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  part_of_data['scores_flair'] = part_of_data['text'].apply(lambda s: score_flair(s)[1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count         9999\n",
       "unique           2\n",
       "top       NEGATIVE\n",
       "freq          7192\n",
       "Name: scores_flair, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sentiment score for each review\n",
    "part_of_data['scores_flair'] = part_of_data['text'].apply(lambda s: score_flair(s)[0])\n",
    "\n",
    "# Predict sentiment label for each review\n",
    "part_of_data['scores_flair'] = part_of_data['text'].apply(lambda s: score_flair(s)[1])\n",
    "\n",
    "# Check the distribution of the score\n",
    "part_of_data['scores_flair'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "789b9697",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:55:19.249399Z",
     "start_time": "2023-01-12T15:55:19.240397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>scores_flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>1550729779</td>\n",
       "      <td>Sat Apr 18 07:05:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>thedoyleswife</td>\n",
       "      <td>Aww that's sad</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>1550730633</td>\n",
       "      <td>Sat Apr 18 07:05:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>gia_revenge</td>\n",
       "      <td>stupid dvds stuffing up the good bits in jaws.</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>1550731192</td>\n",
       "      <td>Sat Apr 18 07:05:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>matmurray</td>\n",
       "      <td>@Dandy_Sephy No. Only close friends and family...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>1550731281</td>\n",
       "      <td>Sat Apr 18 07:05:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lexabuckets</td>\n",
       "      <td>CRAP! After looking when I last tweeted... WHY...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0</td>\n",
       "      <td>1550731500</td>\n",
       "      <td>Sat Apr 18 07:05:32 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmberKarley</td>\n",
       "      <td>Its Another Rainboot day</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      polarity          id                          date     query  \\\n",
       "1            0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2            0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3            0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4            0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5            0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "9995         0  1550729779  Sat Apr 18 07:05:12 PDT 2009  NO_QUERY   \n",
       "9996         0  1550730633  Sat Apr 18 07:05:23 PDT 2009  NO_QUERY   \n",
       "9997         0  1550731192  Sat Apr 18 07:05:29 PDT 2009  NO_QUERY   \n",
       "9998         0  1550731281  Sat Apr 18 07:05:30 PDT 2009  NO_QUERY   \n",
       "9999         0  1550731500  Sat Apr 18 07:05:32 PDT 2009  NO_QUERY   \n",
       "\n",
       "               user                                               text  \\\n",
       "1     scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2          mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3           ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4            Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "5          joy_wolf                      @Kwesidei not the whole crew    \n",
       "...             ...                                                ...   \n",
       "9995  thedoyleswife                                    Aww that's sad    \n",
       "9996    gia_revenge    stupid dvds stuffing up the good bits in jaws.    \n",
       "9997      matmurray  @Dandy_Sephy No. Only close friends and family...   \n",
       "9998    lexabuckets  CRAP! After looking when I last tweeted... WHY...   \n",
       "9999    AmberKarley                          Its Another Rainboot day    \n",
       "\n",
       "     scores_flair  \n",
       "1        NEGATIVE  \n",
       "2        NEGATIVE  \n",
       "3        NEGATIVE  \n",
       "4        NEGATIVE  \n",
       "5        NEGATIVE  \n",
       "...           ...  \n",
       "9995     NEGATIVE  \n",
       "9996     NEGATIVE  \n",
       "9997     NEGATIVE  \n",
       "9998     NEGATIVE  \n",
       "9999     NEGATIVE  \n",
       "\n",
       "[9999 rows x 7 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8af22c09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:55:23.525465Z",
     "start_time": "2023-01-12T15:55:23.514462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NEGATIVE    7192\n",
       "POSITIVE    2807\n",
       "Name: scores_flair, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_of_data['scores_flair'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fbe7b0",
   "metadata": {},
   "source": [
    "2807 sentences labbeled as \"positive\" by flair, despite that they are all tagget manually as \"negative\". Now I want to check what will happen if I use My trained logistic regression model. Will it also labbel them as \"positive\"???  How much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a6ac22d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:55:33.247531Z",
     "start_time": "2023-01-12T15:55:33.242530Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = part_of_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c4783ac6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:55:34.814542Z",
     "start_time": "2023-01-12T15:55:34.794517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       is upset that he can't update his Facebook by ...\n",
       "2       @Kenichan I dived many times for the ball. Man...\n",
       "3         my whole body feels itchy and like its on fire \n",
       "4       @nationwideclass no, it's not behaving at all....\n",
       "5                           @Kwesidei not the whole crew \n",
       "                              ...                        \n",
       "9995                                      Aww that's sad \n",
       "9996      stupid dvds stuffing up the good bits in jaws. \n",
       "9997    @Dandy_Sephy No. Only close friends and family...\n",
       "9998    CRAP! After looking when I last tweeted... WHY...\n",
       "9999                            Its Another Rainboot day \n",
       "Name: text, Length: 9999, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "627e48ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:55:48.481571Z",
     "start_time": "2023-01-12T15:55:46.202615Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_answers_by_log_regression = []\n",
    "for i in texts:\n",
    "    text =[i]\n",
    "    # Transform the text using the same vectorizer\n",
    "    predict_text_train = vectorizer.transform(text)\n",
    "    prediction = model_logistic.predict(predict_text_train)\n",
    "    list_of_answers_by_log_regression.append(int(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "de06634b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T15:55:50.869607Z",
     "start_time": "2023-01-12T15:55:50.859074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " POSITIVE: 3876 NEGATIVE: 6123\n"
     ]
    }
   ],
   "source": [
    "print(f\" POSITIVE: {list_of_answers_by_log_regression.count(4)} NEGATIVE: {list_of_answers_by_log_regression.count(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66fb43",
   "metadata": {},
   "source": [
    "It looks like Flair is much better than logistic regression: 267-357 = 90, 26 percent vs 36 percent.  \n",
    "Next Step I tried to do that on 10000 examples - Flair take much more time, but from 10000 only 2807 (28 percent) mistakes when by logistic regresion, there is 3876 mistakes (39 percent) ~~~~~ 10 percent better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aaefff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T16:00:55.223953Z",
     "start_time": "2023-01-12T16:00:55.211439Z"
    }
   },
   "source": [
    "# ------------Part 3 Hugging Face Zero-shot Sentiment Prediction------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1f2584c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T16:01:50.281867Z",
     "start_time": "2023-01-12T16:01:35.674567Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "classifier = pipeline(task=\"zero-shot-classification\", \n",
    "                      model=\"facebook/bart-large-mnli\",\n",
    "                      device=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "947077b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T16:14:31.658696Z",
     "start_time": "2023-01-12T16:02:55.354757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>labels</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[negative, positive]</td>\n",
       "      <td>[0.9889948964118958, 0.01100502721965313]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[negative, positive]</td>\n",
       "      <td>[0.5754268169403076, 0.42457315325737]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[negative, positive]</td>\n",
       "      <td>[0.9792972803115845, 0.020702756941318512]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[negative, positive]</td>\n",
       "      <td>[0.9944276213645935, 0.005572372581809759]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>[negative, positive]</td>\n",
       "      <td>[0.682918906211853, 0.31708112359046936]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence                labels  \\\n",
       "0  is upset that he can't update his Facebook by ...  [negative, positive]   \n",
       "1  @Kenichan I dived many times for the ball. Man...  [negative, positive]   \n",
       "2    my whole body feels itchy and like its on fire   [negative, positive]   \n",
       "3  @nationwideclass no, it's not behaving at all....  [negative, positive]   \n",
       "4                      @Kwesidei not the whole crew   [negative, positive]   \n",
       "\n",
       "                                       scores  \n",
       "0   [0.9889948964118958, 0.01100502721965313]  \n",
       "1      [0.5754268169403076, 0.42457315325737]  \n",
       "2  [0.9792972803115845, 0.020702756941318512]  \n",
       "3  [0.9944276213645935, 0.005572372581809759]  \n",
       "4    [0.682918906211853, 0.31708112359046936]  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put reviews in a list\n",
    "sequences = part_of_data['text'].to_list()\n",
    "\n",
    "# Define the candidate labels \n",
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "\n",
    "# Set the hyppothesis template\n",
    "hypothesis_template = \"The sentiment of this review is {}.\"\n",
    "\n",
    "# Prediction results\n",
    "hf_prediction = classifier(sequences, candidate_labels, hypothesis_template=hypothesis_template)\n",
    "\n",
    "# Save the output as a dataframe\n",
    "hf_prediction = pd.DataFrame(hf_prediction)\n",
    "\n",
    "# Take a look at the data\n",
    "hf_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "18af836c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T16:17:44.362111Z",
     "start_time": "2023-01-12T16:17:44.346104Z"
    }
   },
   "outputs": [],
   "source": [
    "hf_prediction['hf_prediction'] = hf_prediction['labels'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "24d91667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T16:18:17.205068Z",
     "start_time": "2023-01-12T16:18:17.186062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    8083\n",
       "positive    1916\n",
       "Name: hf_prediction, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_prediction['hf_prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628549a6",
   "metadata": {},
   "source": [
    "As we can see Hugging Face Zero-shot Sentiment is doing much better: only 19.1 percents are mistakes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cd0d2fd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T16:32:38.616669Z",
     "start_time": "2023-01-12T16:32:38.347926Z"
    }
   },
   "outputs": [],
   "source": [
    "sequences = [\"It's too good to be true\"]\n",
    "\n",
    "# Define the candidate labels \n",
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "\n",
    "# Set the hyppothesis template\n",
    "hypothesis_template = \"The sentiment of this review is {}.\"\n",
    "\n",
    "# Prediction results\n",
    "hf_prediction = classifier(sequences, candidate_labels, hypothesis_template=hypothesis_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3bc0acdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T16:32:40.173709Z",
     "start_time": "2023-01-12T16:32:40.155801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': \"It's too good to be true\",\n",
       "  'labels': ['positive', 'negative'],\n",
       "  'scores': [0.8924731016159058, 0.10752695053815842]}]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424cbd58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
